{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damashup/angular-socket-io-im/blob/master/CAS_Script_1_Sorare_Accounting_MyTransactions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "OxIUxTNq_pb3"
      },
      "outputs": [],
      "source": [
        "# Cell 0: CONFIGURATION\n",
        "\n",
        "# Set to True to run the heavy raw data dumps for troubleshooting.\n",
        "# Set to False for faster standard runs (this skips RAW_API_Dump, STANDARDIZED_RAW_Dump, and RAW_CRAFTS_AUDIT).\n",
        "RUN_RAW_DUMPS = False\n",
        "\n",
        "# SCRIPT 1: SNAPSHOT HELPER FUNCTIONS (REVISED TO USE DIRECT DRIVE API)\n",
        "\n",
        "# Globals for Drive service\n",
        "DRIVE_SERVICE = None\n",
        "SNAPSHOT_FOLDER_NAME = \"SorareReportsSnapshots\"\n",
        "\n",
        "def initialize_drive_service():\n",
        "    \"\"\"Initializes the Google Drive API service using gspread's credentials.\"\"\"\n",
        "    global DRIVE_SERVICE\n",
        "    if DRIVE_SERVICE is None:\n",
        "        try:\n",
        "            # Re-authorize to ensure Drive scope is captured\n",
        "            creds, _ = default()\n",
        "            DRIVE_SERVICE = build('drive', 'v3', credentials=creds)\n",
        "            print(\"Drive API service initialized.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to build Drive Service: {e}\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_snapshot_folder_id(folder_name):\n",
        "    \"\"\"Finds the ID of the snapshot folder, creating it if necessary.\"\"\"\n",
        "    if not initialize_drive_service():\n",
        "        return None\n",
        "\n",
        "    # Search for the folder by name in the root folder\n",
        "    q = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and 'root' in parents and trashed=false\"\n",
        "\n",
        "    response = DRIVE_SERVICE.files().list(q=q, spaces='drive', fields='files(id)').execute()\n",
        "    files = response.get('files', [])\n",
        "\n",
        "    if files:\n",
        "        return files[0]['id']\n",
        "    else:\n",
        "        # Create the folder\n",
        "        file_metadata = {\n",
        "            'name': folder_name,\n",
        "            'mimeType': 'application/vnd.google-apps.folder',\n",
        "            'parents': ['root']\n",
        "        }\n",
        "        new_folder = DRIVE_SERVICE.files().create(body=file_metadata, fields='id').execute()\n",
        "        print(f\"Created new Drive folder: '{folder_name}'.\")\n",
        "        return new_folder['id']\n",
        "\n",
        "def create_and_store_snapshot(source_file_id, source_file_name, target_folder_id):\n",
        "    \"\"\"Duplicates the source spreadsheet and moves the copy to the target folder.\"\"\"\n",
        "    if not initialize_drive_service() or not target_folder_id:\n",
        "        print(\"Snapshot failed: Drive service not ready or target folder missing.\")\n",
        "        return\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    snapshot_title = f\"{source_file_name}_Snapshot_{timestamp}\"\n",
        "\n",
        "    try:\n",
        "        # 1. Copy the file\n",
        "        copied_file_metadata = {'name': snapshot_title}\n",
        "        copied_file = DRIVE_SERVICE.files().copy(\n",
        "            fileId=source_file_id,\n",
        "            body=copied_file_metadata,\n",
        "            fields='id, parents'\n",
        "        ).execute()\n",
        "        copied_file_id = copied_file['id']\n",
        "\n",
        "        # 2. Move the copied file to the target folder\n",
        "        # Remove original parent ('root') and add new parent (snapshot folder)\n",
        "\n",
        "        # Get existing parents (usually just 'root')\n",
        "        current_parents = ','.join(copied_file.get('parents'))\n",
        "\n",
        "        DRIVE_SERVICE.files().update(\n",
        "            fileId=copied_file_id,\n",
        "            addParents=target_folder_id,\n",
        "            removeParents=current_parents,\n",
        "            fields='id, parents'\n",
        "        ).execute()\n",
        "\n",
        "        print(f\"✅ Snapshot '{snapshot_title}' successfully created and stored in '{SNAPSHOT_FOLDER_NAME}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Snapshot creation failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "suAmGyTt1zc4"
      },
      "outputs": [],
      "source": [
        "# CELL 1 (UPDATED IMPORTS)\n",
        "\n",
        "# Import the required libraries\n",
        "import requests\n",
        "import csv\n",
        "import datetime\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import pandas as pd # Needed for Drive helper functions\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from google.auth import default # Needed for Drive service auth\n",
        "from googleapiclient.discovery import build # CRITICAL: This defines the 'build' function!\n",
        "\n",
        "# --- END IMPORTS ---\n",
        "\n",
        "\n",
        "# PREREQUISITE\n",
        "def is_drive_mounted(path='/content/drive'):\n",
        "  return os.path.exists(path)\n",
        "\n",
        "if not is_drive_mounted():\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Mq2kwmPr2oMy"
      },
      "outputs": [],
      "source": [
        "# Cell 2 - PREREQUISITE - SORARE\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the key securely from Colab Secrets\n",
        "SORARE_API_KEY = userdata.get('SORARE_API_KEY')\n",
        "SORARE_JWT = userdata.get('SORARE_JWT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "NFArwkiM3X6X"
      },
      "outputs": [],
      "source": [
        "# Cell 3 VARIBLES\n",
        "\n",
        "user = \"nanzo\"\n",
        "\n",
        "# Define the GraphQL endpoint URL\n",
        "url = \"https://api.sorare.com/federation/graphql\"\n",
        "\n",
        "# Define Sorare API key:\n",
        "api_key = SORARE_API_KEY\n",
        "jwt = SORARE_JWT\n",
        "\n",
        "# Define the path to the CSV file in your Google Drive\n",
        "output_csv_path = \"/content/drive/MyDrive/SorareDumps/SA_MyTransactions.csv\"\n",
        "\n",
        "# Define the headers for the CSV files\n",
        "headers = [\n",
        "    \"transaction_type\",\n",
        "    \"card_slug\",                # Card bought OR Card sold\n",
        "    \"card_url\",                 # URL for the card slug\n",
        "    \"sender_slug\",\n",
        "    \"actual_receiver_slug\",\n",
        "    \"token_op_type\",            # <-- NEW COLUMN: The raw GraphQL typename\n",
        "    \"reward_id\",                # <-- NEW COLUMN for Reward Tracking\n",
        "    \"reward_slug\",              # <-- NEW COLUMN for SO5 Reward Slug\n",
        "    \"fiat_value_gbp\",           # Buy Price (Negative) OR Sell Price (Positive)\n",
        "    \"crypto_value_eth\",         # Buy Price (Negative) OR Sell Price (Positive)\n",
        "    \"reference_currency\",\n",
        "    \"cards_exchanged_slugs\",    # Cards received in a trade OR Cards sent in a trade\n",
        "    \"transaction_date\",\n",
        "    \"transaction_time\",\n",
        "    \"status\",\n",
        "    \"pl_value_gbp\",\n",
        "    \"current_time\",\n",
        "    \"account_entry_id\",\n",
        "    \"token_operation_id\" # <--- NEW COLUMN ADDEDD\n",
        "]\n",
        "\n",
        "# Define your Google Sheet names\n",
        "GOOGLE_SHEET_NAME = \"Sorare Transactions Dump\"\n",
        "SALES_WORKSHEET_NAME = \"AccountTransactions_Sales\"\n",
        "BUYS_WORKSHEET_NAME = \"AccountTransactions_Buys\"\n",
        "REWARDS_WORKSHEET_NAME = \"AccountTransactions_MonetaryRewards\"\n",
        "RAW_WORKSHEET_NAME = \"RAW_API_Dump\"\n",
        "STANDARDIZED_RAW_WORKSHEET_NAME = \"STANDARDIZED_RAW_Dump\"\n",
        "ANOMALIES_WORKSHEET_NAME = \"Anomalies_Excluded_Data\"\n",
        "CARD_REWARDS_WORKSHEET_NAME = \"MyRewards_Cards\"\n",
        "CRAFTED_CARDS_WORKSHEET_NAME = \"MyCrafts_Cards\"\n",
        "RAW_CRAFTS_AUDIT_WORKSHEET_NAME = \"RAW_CRAFTS_AUDIT\"\n",
        "NOTIONAL_SALES_WORKSHEET_NAME = \"Notional_Sales_Auto\" # <-- RENAMED SHEET\n",
        "DIRECT_OFFERS_MANUAL_WORKSHEET_NAME = \"Direct_Offers_Manual_Audit\"\n",
        "MANUAL_SALES_WORKSHEET_NAME = \"Notional_Sales_Manual\" # NEW SHEET NAME\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "yIgTQfQC3R8n"
      },
      "outputs": [],
      "source": [
        "# Cell 4: API QUERIES\n",
        "\n",
        "# Define the GraphQL query\n",
        "query = \"\"\"\n",
        "query GetAllTransactionsForUser($cursor: String) {\n",
        "  currentUser {\n",
        "    slug\n",
        "    accountEntries(first: 100, after: $cursor) {\n",
        "      nodes {\n",
        "        id\n",
        "        aasmState\n",
        "        amounts {\n",
        "          gbpCents\n",
        "          referenceCurrency\n",
        "          wei\n",
        "        }\n",
        "        date\n",
        "        provisional\n",
        "        tokenOperation {\n",
        "          ... on TokenOffer {\n",
        "            id\n",
        "            actualReceiver {\n",
        "              ... on User {\n",
        "                slug\n",
        "              }\n",
        "            }\n",
        "            marketFeeAmounts {\n",
        "              gbpCents\n",
        "            }\n",
        "            receiver {\n",
        "              ... on User {\n",
        "                slug\n",
        "              }\n",
        "            }\n",
        "            receiverSide {\n",
        "              amounts {\n",
        "                gbpCents\n",
        "                referenceCurrency\n",
        "                wei\n",
        "              }\n",
        "              anyCards {\n",
        "                slug\n",
        "              }\n",
        "            }\n",
        "            sender {\n",
        "              ... on User {\n",
        "                slug\n",
        "              }\n",
        "            }\n",
        "            senderSide {\n",
        "              amounts {\n",
        "                gbpCents\n",
        "                referenceCurrency\n",
        "                wei\n",
        "              }\n",
        "              anyCards {\n",
        "                slug\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "          ... on TokenBid {\n",
        "            id\n",
        "            userBidder {\n",
        "              slug\n",
        "            }\n",
        "            auction {\n",
        "              anyCards {\n",
        "                slug\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "          ... on TokenPrimaryOffer {\n",
        "            id\n",
        "            buyer {\n",
        "              slug\n",
        "            }\n",
        "            anyCards {\n",
        "              slug\n",
        "            }\n",
        "          }\n",
        "          ... on TokenMonetaryReward {\n",
        "            id\n",
        "            rewardId\n",
        "            sport\n",
        "          }\n",
        "          ... on So5Reward {\n",
        "            id\n",
        "            slug\n",
        "            rewardCards {\n",
        "              anyCard {\n",
        "                slug\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "          __typename\n",
        "        }\n",
        "      }\n",
        "      pageInfo {\n",
        "        endCursor\n",
        "        hasNextPage\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "9A0V0L-T3idw"
      },
      "outputs": [],
      "source": [
        "# Cell 5 REWARDS API QUERY\n",
        "\n",
        "# Define the rewards GraphQL query\n",
        "rewards_query = \"\"\"\n",
        "query MyRewards($slug: String!, $cursor: String) {\n",
        "  user(slug: $slug) {\n",
        "    rewardedRankings(first: 100, after: $cursor) {\n",
        "      nodes {\n",
        "        so5Rewards {\n",
        "          id\n",
        "          slug\n",
        "          so5Leaderboard {\n",
        "            gameWeek\n",
        "            endDate\n",
        "          }\n",
        "          amount {\n",
        "            gbpCents\n",
        "            referenceCurrency\n",
        "            wei\n",
        "          }\n",
        "          rewardCards {\n",
        "            anyCard {\n",
        "              slug\n",
        "            }\n",
        "          }\n",
        "\t\t\t\t\trewards {\n",
        "            id\n",
        "            ... on AnyCardReward {\n",
        "              id\n",
        "              aasmState\n",
        "            }\n",
        "            ... on MonetaryReward {\n",
        "              id\n",
        "              aasmState\n",
        "            }\n",
        "            __typename\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      pageInfo {\n",
        "        endCursor\n",
        "        hasNextPage\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Define the crafts GraphQL query (CRITICAL ADDITION)\n",
        "crafts_query = \"\"\"\n",
        "query MyCrafts($cursor: String, $rarity: Rarity!) {\n",
        "  currentUser {\n",
        "    slug\n",
        "    cardShardsHistoryTransactions(\n",
        "      first: 100\n",
        "      after: $cursor\n",
        "      rarity: $rarity\n",
        "      sport: FOOTBALL\n",
        "    ) {\n",
        "      nodes {\n",
        "        date\n",
        "        description\n",
        "        id\n",
        "        label\n",
        "      }\n",
        "      pageInfo {\n",
        "        endCursor\n",
        "        hasNextPage\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- NEW: Direct Offers GraphQL Query ---\n",
        "direct_offers_query = \"\"\"\n",
        "query GetAllDirectOffersForUser($direction: OfferDirection!, $cursor: String, $first: Int) {\n",
        "    currentUser {\n",
        "      slug\n",
        "      tokenOffers(direction: $direction, first: $first, after: $cursor,states:ACCEPTED) {\n",
        "        totalCount\n",
        "        nodes {\n",
        "          id\n",
        "          status\n",
        "          acceptedAt\n",
        "          receiver {\n",
        "            ... on User {\n",
        "              slug\n",
        "            }\n",
        "          }\n",
        "          receiverSide {\n",
        "            amounts {\n",
        "              gbpCents\n",
        "              wei\n",
        "              referenceCurrency\n",
        "            }\n",
        "            anyCards {\n",
        "              slug\n",
        "            }\n",
        "          }\n",
        "          sender {\n",
        "            ... on User {\n",
        "              slug\n",
        "            }\n",
        "          }\n",
        "          senderSide {\n",
        "            amounts {\n",
        "              gbpCents\n",
        "              wei\n",
        "              referenceCurrency\n",
        "            }\n",
        "            anyCards {\n",
        "              slug\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "        pageInfo {\n",
        "          endCursor\n",
        "          hasNextPage\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XjkEU2B-34QZ"
      },
      "outputs": [],
      "source": [
        "# Cell 6: REWARDS API FETCH LOOP (Updated to handle new query structure)\n",
        "\n",
        "def fetch_all_rewards(query, user_slug, api_key, jwt):\n",
        "    all_rewards = []\n",
        "    variables = {\"slug\": user_slug, \"cursor\": None}\n",
        "    url = \"https://api.sorare.com/federation/graphql\" # Use global URL\n",
        "    has_next_page = True\n",
        "\n",
        "    # Set up the request headers (copied from execute_query)\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"APIKEY\": api_key,\n",
        "        \"JWT-AUD\": \"MyNanzo\",\n",
        "        \"Authorization\": \"Bearer \" + jwt\n",
        "    }\n",
        "\n",
        "    print(\"Fetching rewards data...\")\n",
        "\n",
        "    while has_next_page:\n",
        "        payload = {\"query\": query, \"variables\": variables}\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        print(f\"Status Code: {response.status_code}\")\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Stopping rewards fetch due to HTTP error {response.status_code}.\")\n",
        "            break\n",
        "\n",
        "        response_json = response.json()\n",
        "\n",
        "        if 'errors' in response_json:\n",
        "            print(f\"GraphQL Errors: {response_json['errors']}\")\n",
        "            break\n",
        "\n",
        "        # Navigate to the correct nested nodes array\n",
        "        try:\n",
        "            # The structure is: data -> user -> rewardedRankings\n",
        "            rewarded_rankings = response_json[\"data\"][\"user\"][\"rewardedRankings\"]\n",
        "            current_nodes = rewarded_rankings[\"nodes\"]\n",
        "            page_info = rewarded_rankings[\"pageInfo\"]\n",
        "        except KeyError as e:\n",
        "            # Capture the full JSON output for inspection\n",
        "            print(f\"Error: Could not find key '{e}' in response. Stopping.\")\n",
        "            # We already have the JSON in the response_json variable from above\n",
        "            print(f\"Full JSON: {response_json}\")\n",
        "            break\n",
        "\n",
        "        all_rewards.extend(current_nodes)\n",
        "\n",
        "        variables[\"cursor\"] = page_info[\"endCursor\"]\n",
        "        has_next_page = page_info[\"hasNextPage\"]\n",
        "\n",
        "    print(f\"Total rewards entries fetched: {len(all_rewards)}\")\n",
        "    return all_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "lrnPsmFoLE9F"
      },
      "outputs": [],
      "source": [
        "# Cell 7: CRAFTS API FETCH LOOP (Case Sensitivity Fix)\n",
        "\n",
        "def fetch_all_crafts(query, api_key, jwt):\n",
        "    all_crafts = []\n",
        "    # Rarity names must be lowercase as expected by the API\n",
        "    rarities = [\"unique\", \"super_rare\", \"rare\", \"limited\"]\n",
        "    url = \"https://api.sorare.com/federation/graphql\"\n",
        "\n",
        "    # Set up headers once\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"APIKEY\": api_key,\n",
        "        \"JWT-AUD\": \"MyNanzo\",\n",
        "        \"Authorization\": \"Bearer \" + jwt\n",
        "    }\n",
        "\n",
        "    print(\"\\nFetching crafted card data for all rarities...\")\n",
        "\n",
        "    for rarity in rarities:\n",
        "        current_crafts = []\n",
        "        # FIX: Send rarity in lowercase (e.g., \"unique\" instead of \"UNIQUE\")\n",
        "        variables = {\"cursor\": None, \"rarity\": rarity}\n",
        "        has_next_page = True\n",
        "\n",
        "        print(f\"  -> Fetching {rarity} crafts...\")\n",
        "\n",
        "        while has_next_page:\n",
        "            payload = {\"query\": query, \"variables\": variables}\n",
        "            response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"    Stopping fetch due to HTTP error {response.status_code}.\")\n",
        "                break\n",
        "\n",
        "            response_json = response.json()\n",
        "\n",
        "            if 'errors' in response_json:\n",
        "                print(f\"    GraphQL Errors: {response_json['errors']}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Structure: data -> currentUser -> cardShardsHistoryTransactions\n",
        "                craft_data = response_json[\"data\"][\"currentUser\"][\"cardShardsHistoryTransactions\"]\n",
        "                current_nodes = craft_data[\"nodes\"]\n",
        "                page_info = craft_data[\"pageInfo\"]\n",
        "            except KeyError:\n",
        "                print(\"    Error: Craft data structure is unexpected. Stopping.\")\n",
        "                break\n",
        "\n",
        "            current_crafts.extend(current_nodes)\n",
        "\n",
        "            variables[\"cursor\"] = page_info[\"endCursor\"]\n",
        "            has_next_page = page_info[\"hasNextPage\"]\n",
        "\n",
        "        all_crafts.extend(current_crafts)\n",
        "\n",
        "    print(f\"Total crafted transactions fetched: {len(all_crafts)}\")\n",
        "    return all_crafts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "7xfoCH-fr9Wd"
      },
      "outputs": [],
      "source": [
        "# Cell 8: DIRECT OFFERS API FETCH LOOP\n",
        "\n",
        "def fetch_all_direct_offers(query, user_slug, api_key, jwt):\n",
        "    \"\"\"\n",
        "    Fetches all SENT and RECEIVED direct offers and returns only those with 'accepted' status.\n",
        "    \"\"\"\n",
        "    all_accepted_offers = []\n",
        "    directions = [\"SENT\", \"RECEIVED\"] # Fetch both directions\n",
        "    url = \"https://api.sorare.com/federation/graphql\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"APIKEY\": api_key,\n",
        "        \"JWT-AUD\": \"MyNanzo\",\n",
        "        \"Authorization\": \"Bearer \" + jwt\n",
        "    }\n",
        "\n",
        "    print(\"\\nFetching ACCEPTED direct offer data for SENT and RECEIVED directions...\")\n",
        "\n",
        "    for direction in directions:\n",
        "        # Note: The 'states:ACCEPTED' filter is now in the query itself.\n",
        "        # We need to set 'first' parameter explicitly as per the query structure.\n",
        "        variables = {\"direction\": direction, \"cursor\": None, \"first\": 100}\n",
        "        has_next_page = True\n",
        "        current_offers_count = 0\n",
        "\n",
        "        print(f\"  -> Fetching {direction} offers...\")\n",
        "\n",
        "        while has_next_page:\n",
        "            payload = {\"query\": query, \"variables\": variables}\n",
        "            response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"    Stopping fetch due to HTTP error {response.status_code}.\")\n",
        "                break\n",
        "\n",
        "            response_json = response.json()\n",
        "\n",
        "            if 'errors' in response_json:\n",
        "                print(f\"    GraphQL Errors: {response_json['errors']}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                offer_data = response_json[\"data\"][\"currentUser\"][\"tokenOffers\"]\n",
        "                current_nodes = offer_data[\"nodes\"]\n",
        "                page_info = offer_data[\"pageInfo\"]\n",
        "            except KeyError:\n",
        "                print(\"    Error: Direct offer data structure is unexpected. Stopping.\")\n",
        "                break\n",
        "\n",
        "            # Store all nodes, as they are pre-filtered by the API query\n",
        "            for offer in current_nodes:\n",
        "                offer['direction_role'] = direction\n",
        "                all_accepted_offers.append(offer)\n",
        "                current_offers_count += 1\n",
        "\n",
        "            variables[\"cursor\"] = page_info[\"endCursor\"]\n",
        "            has_next_page = page_info[\"hasNextPage\"]\n",
        "\n",
        "        print(f\"  -> Found {current_offers_count} accepted {direction} offers.\")\n",
        "\n",
        "\n",
        "    print(f\"Total accepted direct offers fetched: {len(all_accepted_offers)}\")\n",
        "    return all_accepted_offers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "S8iI12Dv5LRv"
      },
      "outputs": [],
      "source": [
        "# CEll 9 FUNCTIONS\n",
        "\n",
        "#SORARE API CALL\n",
        "# Define a function to execute the query\n",
        "def execute_query(query, variables, api_key, jwt):\n",
        "    # Set up the request headers and payload\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"APIKEY\": api_key,\n",
        "        \"JWT-AUD\": \"MyNanzo\",\n",
        "        \"Authorization\": \"Bearer \" + jwt\n",
        "    }\n",
        "    payload = {\"query\": query, \"variables\": variables}\n",
        "\n",
        "    # Send the request to the API endpoint\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "\n",
        "    response_json = response.json()\n",
        "\n",
        "    # Check for GraphQL errors\n",
        "    if 'errors' in response_json:\n",
        "        print(f\"GraphQL Errors: {response_json['errors']}\")\n",
        "        return None\n",
        "\n",
        "    # Parse the response JSON\n",
        "    try:\n",
        "        # Navigate to the 'accountEntries' object directly\n",
        "        account_entries = response_json[\"data\"][\"currentUser\"][\"accountEntries\"]\n",
        "    except KeyError as e:\n",
        "        # Print the full response for better debugging if the expected structure is missing\n",
        "        print(f\"Error: Could not find key in response. Full JSON: {response_json}\")\n",
        "        raise e\n",
        "\n",
        "    return account_entries # account_entries is {'nodes': [...], 'pageInfo': {...}}\n",
        "\n",
        "\n",
        "# Helper function to safely get a nested value\n",
        "def get_nested(data, keys, default=None):\n",
        "   # Safely retrieves a nested value from a dictionary\n",
        "   if data is None:\n",
        "       return default\n",
        "   value = data\n",
        "   for key in keys:\n",
        "       if isinstance(value, dict) and key in value:\n",
        "           value = value[key]\n",
        "       elif isinstance(value, list) and isinstance(key, int) and key < len(value):\n",
        "           value = value[key]\n",
        "       else:\n",
        "           return default\n",
        "   return value\n",
        "\n",
        "\n",
        "# Function to convert Wei (as a string) to ETH (as a float)\n",
        "def wei_to_eth(wei_str):\n",
        "   \"\"\"Converts a wei string value to a floating point ETH value.\"\"\"\n",
        "   try:\n",
        "       # Convert the string to an integer\n",
        "       wei = int(wei_str)\n",
        "       # 1 ETH = 10^18 Wei\n",
        "       eth = wei / 10**18\n",
        "       # Round to 8 decimal places for reasonable precision\n",
        "       return round(eth, 8)\n",
        "   except (ValueError, TypeError):\n",
        "       return 0.0\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# --- WRITING FUNCTIONS (Updated Overwrite Logic) ---\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def write_raw_data(transactions_list, sheet_name, raw_worksheet_name):\n",
        "    import json\n",
        "    raw_headers = [\"ID\", \"Raw JSON Data\"]\n",
        "\n",
        "    try:\n",
        "        sh = gc.open(sheet_name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        print(f\"Spreadsheet '{sheet_name}' not found. Skipping raw data dump.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        wks_raw = sh.worksheet(raw_worksheet_name)\n",
        "        wks_raw.clear() # <--- FIX: Clear existing content if sheet is found\n",
        "    except gspread.WorksheetNotFound:\n",
        "        wks_raw = sh.add_worksheet(title=raw_worksheet_name, rows=\"100\", cols=\"2\")\n",
        "\n",
        "    # Prepare data\n",
        "    rows_to_write = []\n",
        "    for i, tx in enumerate(transactions_list):\n",
        "        transaction_id = tx.get('id', f'ROW_{i+1}')\n",
        "        raw_json_string = json.dumps(tx)\n",
        "        rows_to_write.append([transaction_id, raw_json_string])\n",
        "\n",
        "    data_to_overwrite = [raw_headers] + rows_to_write\n",
        "\n",
        "    if data_to_overwrite:\n",
        "        num_rows = len(data_to_overwrite)\n",
        "        num_cols = len(raw_headers)\n",
        "        range_label = f'A1:{gspread.utils.rowcol_to_a1(num_rows, num_cols)}'\n",
        "\n",
        "        wks_raw.update(range_name=range_label, values=data_to_overwrite, value_input_option='RAW')\n",
        "\n",
        "        print(f\"Successfully OVERWROTE {len(rows_to_write)} rows of RAW JSON data to '{raw_worksheet_name}'.\")\n",
        "    else:\n",
        "        # Clear and write headers if no data\n",
        "        wks_raw.clear()\n",
        "        wks_raw.append_row(raw_headers)\n",
        "        print(\"No raw data to dump to Google Sheet. Headers were refreshed.\")\n",
        "\n",
        "\n",
        "def write_standardized_raw_data(transactions_list, sheet_name, standardized_worksheet_name):\n",
        "    standardized_headers = [\n",
        "        \"ID\", \"AASM_State\", \"Date_Time\", \"GBP_Cents\", \"Ref_Currency\", \"WEI_Amount\",\n",
        "        \"TokenOp_Type\", \"TokenOp_Sender\", \"TokenOp_Receiver\", \"TokenOp_ActualReceiver\",\n",
        "        \"SenderSide_Cards\", \"ReceiverSide_Cards\", \"Auction_Card_Slug\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        sh = gc.open(sheet_name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        print(f\"Spreadsheet '{sheet_name}' not found. Skipping standardized raw data dump.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        wks_std = sh.worksheet(standardized_worksheet_name)\n",
        "        wks_std.clear() # <--- FIX: Clear existing content if sheet is found\n",
        "    except gspread.WorksheetNotFound:\n",
        "        wks_std = sh.add_worksheet(title=standardized_worksheet_name, rows=\"100\", cols=str(len(standardized_headers)))\n",
        "\n",
        "\n",
        "    rows_to_write = []\n",
        "    for i, tx in enumerate(transactions_list):\n",
        "        token_op = tx.get('tokenOperation', {}) if tx.get('tokenOperation') is not None else {}\n",
        "        # ... (populating rows_to_write remains the same)\n",
        "        rows_to_write.append([\n",
        "            tx.get('id', f'ROW_{i+1}'), tx.get('aasmState', 'N/A'), tx.get('date', 'N/A'),\n",
        "            get_nested(tx, ['amounts', 'gbpCents']), get_nested(tx, ['amounts', 'referenceCurrency']), get_nested(tx, ['amounts', 'wei']),\n",
        "            token_op.get('__typename', 'N/A'), get_nested(token_op, ['sender', 'slug'], 'N/A'),\n",
        "            get_nested(token_op, ['receiver', 'slug'], 'N/A'), get_nested(token_op, ['actualReceiver', 'slug'], 'N/A'),\n",
        "            \", \".join([c['slug'] for c in get_nested(token_op, ['senderSide', 'anyCards'], [])]),\n",
        "            \", \".join([c['slug'] for c in get_nested(token_op, ['receiverSide', 'anyCards'], [])]),\n",
        "            \", \".join([c['slug'] for c in get_nested(token_op, ['auction', 'anyCards'], [])])\n",
        "        ])\n",
        "\n",
        "    data_to_overwrite = [standardized_headers] + rows_to_write\n",
        "\n",
        "    if data_to_overwrite:\n",
        "        num_rows = len(data_to_overwrite)\n",
        "        num_cols = len(standardized_headers)\n",
        "        range_label = f'A1:{gspread.utils.rowcol_to_a1(num_rows, num_cols)}'\n",
        "\n",
        "        wks_std.update(range_name=range_label, values=data_to_overwrite, value_input_option='RAW')\n",
        "\n",
        "        print(f\"Successfully OVERWROTE {len(rows_to_write)} rows of STANDARDIZED RAW data to '{standardized_worksheet_name}'.\")\n",
        "    else:\n",
        "        wks_std.clear()\n",
        "        wks_std.append_row(standardized_headers)\n",
        "        print(\"No standardized raw data to dump to Google Sheet. Headers were refreshed.\")\n",
        "\n",
        "\n",
        "def write_to_google_sheet(data_rows, sheet_name, worksheet_name, headers):\n",
        "    # ... (Code to open spreadsheet and get/create worksheet remains the same) ...\n",
        "    try:\n",
        "        sh = gc.open(sheet_name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        print(f\"Spreadsheet '{sheet_name}' not found. Please create it manually.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        wks = sh.worksheet(worksheet_name)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        wks = sh.add_worksheet(title=worksheet_name, rows=\"100\", cols=str(len(headers)))\n",
        "        wks.append_row(headers)\n",
        "\n",
        "    else: # If sheet exists, clear it for overwrite\n",
        "        # NOTE: Clearing here is redundant since update() overwrites, but is kept\n",
        "        # as a safety measure for small ranges if update fails.\n",
        "        pass\n",
        "\n",
        "    if data_rows:\n",
        "        data_to_overwrite = [headers] + data_rows\n",
        "        num_rows = len(data_to_overwrite)\n",
        "        num_cols = len(headers)\n",
        "        range_label = f'A1:{gspread.utils.rowcol_to_a1(num_rows, num_cols)}'\n",
        "\n",
        "        # FIX: Use named arguments to resolve DeprecationWarning\n",
        "        wks.update(range_name=range_label, values=data_to_overwrite, value_input_option='RAW')\n",
        "\n",
        "        print(f\"Successfully OVERWROTE {len(data_rows)} rows (including headers) to '{worksheet_name}' in '{sheet_name}'.\")\n",
        "    else:\n",
        "        wks.clear()\n",
        "        wks.append_row(headers)\n",
        "        print(f\"No data to write to {worksheet_name}. Headers were refreshed.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# --- END WRITING FUNCTIONS ---\n",
        "# -------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "E7Q3ckze2MIp"
      },
      "outputs": [],
      "source": [
        "# Cell 9.5: BATCH WRITING FUNCTION (FINAL SYNTAX FIX USING PARAMS)\n",
        "\n",
        "def write_in_batch(google_sheet_name, all_requests_map):\n",
        "    \"\"\"\n",
        "    Performs all necessary Google Sheet updates in a single batch call.\n",
        "\n",
        "    Args:\n",
        "        google_sheet_name (str): Name of the target Google Spreadsheet.\n",
        "        all_requests_map (dict): Keys are worksheet names (str).\n",
        "                                 Values are a tuple: (list of headers, list of data rows).\n",
        "    \"\"\"\n",
        "    import gspread.utils\n",
        "\n",
        "    try:\n",
        "        sh = gc.open(google_sheet_name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        print(f\"Error: Spreadsheet '{google_sheet_name}' not found. Cannot perform batch update.\")\n",
        "        return\n",
        "\n",
        "    requests_body = [] # This will hold the body structure for the API call\n",
        "\n",
        "    for worksheet_name, (headers, data_rows) in all_requests_map.items():\n",
        "        # 1. Get or Create the Worksheet\n",
        "        try:\n",
        "            wks = sh.worksheet(worksheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            wks = sh.add_worksheet(title=worksheet_name, rows=\"100\", cols=str(len(headers)))\n",
        "            print(f\"-> Created new sheet: {worksheet_name}\")\n",
        "\n",
        "        # 2. Prepare data for overwrite (Headers + Data)\n",
        "        if data_rows:\n",
        "            data_to_overwrite = [headers] + data_rows\n",
        "            num_rows = len(data_to_overwrite)\n",
        "            num_cols = len(headers)\n",
        "            # Range must be prefixed with the sheet name!\n",
        "            range_label = f'{worksheet_name}!A1:{gspread.utils.rowcol_to_a1(num_rows, num_cols)}'\n",
        "\n",
        "            # Append the required keys for the API call structure\n",
        "            requests_body.append({\n",
        "                'range': range_label,\n",
        "                'values': data_to_overwrite\n",
        "            })\n",
        "            print(f\"-> Prepared {len(data_rows)} rows for '{worksheet_name}'.\")\n",
        "        else:\n",
        "            # If no data, clear the sheet and write headers (still one write)\n",
        "            wks.clear()\n",
        "            wks.append_row(headers)\n",
        "            print(f\"-> No data for '{worksheet_name}'. Refreshed headers.\")\n",
        "\n",
        "    # 3. Execute Batch Update (consolidated API call)\n",
        "    if requests_body:\n",
        "        try:\n",
        "            # FIX: Use sh.client.request to access the underlying API call directly,\n",
        "            # passing the value_input_option in the body parameter map (v4 sheets API standard)\n",
        "\n",
        "            # The URL to batch update sheet values is:\n",
        "            # /v4/spreadsheets/{spreadsheetId}/values:batchUpdate\n",
        "\n",
        "            # Prepare the full JSON body payload\n",
        "            payload = {\n",
        "                'valueInputOption': 'RAW',\n",
        "                'data': requests_body\n",
        "            }\n",
        "\n",
        "            sh.client.request(\n",
        "                'post',\n",
        "                f'https://sheets.googleapis.com/v4/spreadsheets/{sh.id}/values:batchUpdate',\n",
        "                json=payload\n",
        "            )\n",
        "\n",
        "            print(\"\\n✅ Successfully executed BATCH UPDATE for all sheets.\")\n",
        "        except Exception as e:\n",
        "            # If the raw request fails, try the internal gspread method just in case\n",
        "            print(f\"\\n❌ ERROR during direct batch update call: {e}\")\n",
        "\n",
        "            try:\n",
        "                 sh.values_batch_update(\n",
        "                    requests_body,\n",
        "                    value_input_option='RAW'\n",
        "                )\n",
        "                 print(\"\\n✅ Successfully executed fallback BATCH UPDATE for all sheets.\")\n",
        "            except Exception as e:\n",
        "                print(f\"\\n❌ ERROR: Fallback batch update also failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "collapsed": true,
        "id": "0NoFmkfpIYOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727fcd05-889e-42d9-b1aa-1e201a935534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.10.5)\n",
            "Google Sheets API access granted successfully.\n"
          ]
        }
      ],
      "source": [
        "# CELL 10 PREREQUISITE - GOOGLE SHEETS API\n",
        "!pip install gspread\n",
        "# REMOVE: from google.colab import auth\n",
        "# REMOVE: from google.auth import default\n",
        "import gspread\n",
        "\n",
        "# Authenticate with your user account\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Get credentials and authorize gspread\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "print(\"Google Sheets API access granted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "collapsed": true,
        "id": "IvQ8XazTO5FQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ba08f7-0711-4bdd-cd1b-1638536e8e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "\n",
            "8174 accountEntries records fetched from Sorare API. Query took 97 seconds\n",
            "Total raw transactions retrieved: 8174 (Full API Dump)\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: API FETCH LOOP (RAW DATA RETRIEVAL - NO FILTERING)\n",
        "\n",
        "# Define a list to hold the transactions\n",
        "transactions = []\n",
        "\n",
        "# Define the variables for the query\n",
        "variables = {\n",
        "   \"cursor\": None,\n",
        "}\n",
        "\n",
        "fetch_counter = 0\n",
        "fetch_start_time = time.time()\n",
        "\n",
        "has_next_page = True # Initialize loop condition\n",
        "\n",
        "# Loop through the pages of results\n",
        "while has_next_page:\n",
        "    # Execute the query. 'account_entries' now holds the 'accountEntries' object directly.\n",
        "    account_entries = execute_query(query, variables, api_key, jwt)\n",
        "\n",
        "    if account_entries is None:\n",
        "        print(\"Stopping fetch due to API error or no data.\")\n",
        "        break\n",
        "\n",
        "    current_nodes = account_entries[\"nodes\"]\n",
        "    page_info = account_entries[\"pageInfo\"]\n",
        "\n",
        "    # Add ALL new transactions to the global list (NO DEDUPLICATION HERE)\n",
        "    transactions += current_nodes\n",
        "\n",
        "    variables[\"cursor\"] = page_info[\"endCursor\"]\n",
        "    has_next_page = page_info[\"hasNextPage\"]\n",
        "    fetch_counter += len(current_nodes)\n",
        "\n",
        "# Get the current time after executing the query\n",
        "fetch_end_time = time.time()\n",
        "fetch_elapsed_time = math.ceil(fetch_end_time - fetch_start_time)\n",
        "print(f\"\\n{fetch_counter} accountEntries records fetched from Sorare API. Query took {fetch_elapsed_time} seconds\")\n",
        "print(f\"Total raw transactions retrieved: {len(transactions)} (Full API Dump)\")\n",
        "\n",
        "# The 'transactions' list now contains ALL raw entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "SYBAkOR9Korx"
      },
      "outputs": [],
      "source": [
        "# Cell 12: DATA TRANSFORMATION AND EXPORT LOGIC (Indentation Corrected)\n",
        "\n",
        "# Define new anomaly sheet name (Moved from last cell for function usage)\n",
        "ANOMALIES_WORKSHEET_NAME = \"Anomalies_Excluded_Data\"\n",
        "\n",
        "\n",
        "# Helper function to safely get a nested value (KEEP THIS)\n",
        "def get_nested(data, keys, default=None):\n",
        "   # Safely retrieves a nested value from a dictionary\n",
        "   if data is None:\n",
        "       return default\n",
        "   value = data\n",
        "   for key in keys:\n",
        "       if isinstance(value, dict) and key in value:\n",
        "           value = value[key]\n",
        "       elif isinstance(value, list) and isinstance(key, int) and key < len(value):\n",
        "           value = value[key]\n",
        "       else:\n",
        "           return default\n",
        "   return value\n",
        "\n",
        "\n",
        "# Function to convert Wei (as a string) to ETH (as a float) (KEEP THIS)\n",
        "def wei_to_eth(wei_str):\n",
        "   \"\"\"Converts a wei string value to a floating point ETH value.\"\"\"\n",
        "   try:\n",
        "       wei = int(wei_str)\n",
        "       eth = wei / 10**18\n",
        "       return round(eth, 8)\n",
        "   except (ValueError, TypeError):\n",
        "       return 0.0\n",
        "\n",
        "\n",
        "# --- NEW FILTERING FUNCTION ---\n",
        "def filter_transactions(transactions_list):\n",
        "    \"\"\"Filters transactions into a main list (GBP/WEI) and an anomalies list (other currencies).\"\"\"\n",
        "    main_list = []\n",
        "    anomaly_list = []\n",
        "\n",
        "    for tx in transactions_list:\n",
        "        reference_currency = get_nested(tx, ['amounts', 'referenceCurrency'])\n",
        "\n",
        "        if reference_currency in [\"GBP\", \"WEI\"]:\n",
        "            main_list.append(tx)\n",
        "        else:\n",
        "            anomaly_list.append(tx)\n",
        "\n",
        "    return main_list, anomaly_list\n",
        "\n",
        "\n",
        "# --- UPDATED MAIN TRANSFORMATION FUNCTION (WITH REWARD_ID) ---\n",
        "def transform_transaction_data(transactions, user_slug, headers):\n",
        "    transformed_data = []\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    for tx in transactions:\n",
        "        gbp_cents = get_nested(tx, ['amounts', 'gbpCents'], 0)\n",
        "        aasm_state = tx.get('aasmState')\n",
        "        account_entry_id = tx.get('id', '') # <--- EXTRACT ID HERE\n",
        "\n",
        "        token_op = tx.get('tokenOperation')\n",
        "\n",
        "        # 🔥 FIX: Explicitly use get_nested to retrieve the TokenOperation ID.\n",
        "        # This handles nested structure variations more reliably.\n",
        "        token_operation_id = get_nested(token_op, ['id'], '')\n",
        "\n",
        "        if token_op is None or aasm_state != 'CONFIRMED':\n",
        "            continue\n",
        "\n",
        "\n",
        "        op_type = token_op.get('__typename')\n",
        "\n",
        "        # Initialize variables\n",
        "        tx_type = \"OTHER\"\n",
        "        main_card_slug = \"\"\n",
        "        cards_exchanged_str = \"\"\n",
        "        sender = 'N/A'\n",
        "        receiver = 'N/A'\n",
        "        reward_id = \"\"  # <-- Initialize reward_id\n",
        "\n",
        "        is_classified = False\n",
        "\n",
        "        # --- SCENARIO 1: TokenOffer (Sale, Offer Buy, Part Exchange) ---\n",
        "        if op_type == 'TokenOffer':\n",
        "\n",
        "            sender_slug = get_nested(token_op, ['sender', 'slug'], 'N/A')\n",
        "            receiver_slug = get_nested(token_op, ['actualReceiver', 'slug'])\n",
        "\n",
        "            cards_on_sender_side = [c['slug'] for c in get_nested(token_op, ['senderSide', 'anyCards'], [])]\n",
        "            cards_on_receiver_side = [c['slug'] for c in get_nested(token_op, ['receiverSide', 'anyCards'], [])]\n",
        "\n",
        "\n",
        "            # SALE LOGIC (Type 1)\n",
        "            if gbp_cents > 0 and sender_slug == user_slug:\n",
        "                tx_type = \"SALE\"\n",
        "                main_card_slug = \", \".join(cards_on_sender_side)\n",
        "                cards_exchanged_str = \", \".join(cards_on_receiver_side)\n",
        "                sender = sender_slug\n",
        "                receiver = receiver_slug\n",
        "                is_classified = True\n",
        "\n",
        "            # BUY LOGIC 1 & 2 (User is Payer: gbpCents < 0)\n",
        "            elif gbp_cents < 0:\n",
        "                # Determine user role and corresponding card sides\n",
        "                if sender_slug == user_slug:\n",
        "                    cards_user_sent = cards_on_sender_side\n",
        "                    cards_user_received = cards_on_receiver_side\n",
        "                    seller_slug = receiver_slug\n",
        "                    buyer_slug = sender_slug\n",
        "                elif receiver_slug == user_slug:\n",
        "                    cards_user_sent = cards_on_receiver_side\n",
        "                    cards_user_received = cards_on_sender_side\n",
        "                    seller_slug = sender_slug\n",
        "                    buyer_slug = receiver_slug\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                is_part_exchange = len(cards_user_sent) > 0\n",
        "                tx_type = \"BUY (Part Exchange)\" if is_part_exchange else (\"BUY (Offer Sent)\" if sender_slug == user_slug else \"BUY (Offer)\")\n",
        "\n",
        "                main_card_slug = \", \".join(cards_user_received)\n",
        "                cards_exchanged_str = \", \".join(cards_user_sent)\n",
        "                sender = seller_slug\n",
        "                receiver = buyer_slug\n",
        "                is_classified = True\n",
        "\n",
        "        # --- SCENARIO 2: TokenBid (Auction Buy) ---\n",
        "        elif op_type == 'TokenBid' and gbp_cents < 0:\n",
        "            tx_type = \"BUY (Auction)\"\n",
        "            main_card_slug = \", \".join([c['slug'] for c in get_nested(token_op, ['auction', 'anyCards'], [])])\n",
        "            sender = 'Sorare'\n",
        "            receiver = user_slug\n",
        "            is_classified = True\n",
        "\n",
        "        # --- SCENARIO 3: TokenPrimaryOffer (Instant Buy) ---\n",
        "        elif op_type == 'TokenPrimaryOffer' and gbp_cents < 0:\n",
        "            tx_type = \"BUY (Instant)\"\n",
        "            main_card_slug = \", \".join([c['slug'] for c in get_nested(token_op, ['anyCards'], [])])\n",
        "            sender = 'Sorare'\n",
        "            receiver = get_nested(token_op, ['buyer', 'slug'], user_slug)\n",
        "            is_classified = True\n",
        "\n",
        "        # --- SCENARIO 4: MONETARY REWARDS (New) ---\n",
        "        elif op_type == 'TokenMonetaryReward' and gbp_cents > 0:\n",
        "            tx_type = \"REWARD (Monetary)\"\n",
        "            main_card_slug = \"\"\n",
        "            sender = 'Sorare'\n",
        "            receiver = user_slug\n",
        "            reward_id = get_nested(token_op, ['rewardId'], 'N/A') # <-- Mapped from rewardId\n",
        "            is_classified = True\n",
        "\n",
        "        elif op_type == 'So5Reward' and gbp_cents > 0:\n",
        "            tx_type = \"REWARD (Legacy)\"\n",
        "\n",
        "            # Extract potential card info (as before)\n",
        "            reward_cards = get_nested(token_op, ['rewardCards'], [])\n",
        "            card_slugs = []\n",
        "            for item in reward_cards:\n",
        "                if 'anyCard' in item and 'slug' in item['anyCard']:\n",
        "                     card_slugs.append(item['anyCard']['slug'])\n",
        "\n",
        "            main_card_slug = \", \".join(card_slugs)\n",
        "            sender = 'Sorare'\n",
        "            receiver = user_slug\n",
        "            reward_id = get_nested(token_op, ['slug'], 'N/A') # <-- Mapped from slug\n",
        "            is_classified = True\n",
        "\n",
        "\n",
        "        if not is_classified:\n",
        "            continue\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Data Mapping and Currency Formatting\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "        card_url = f\"https://sorare.com/cards/{main_card_slug.split(',')[0].strip()}\" if main_card_slug and main_card_slug != \"\" else \"\"\n",
        "\n",
        "        fiat_value_gbp = gbp_cents / 100.0\n",
        "        reference_currency = get_nested(tx, ['amounts', 'referenceCurrency'], 'N/A')\n",
        "        sell_price_wei_str = get_nested(tx, ['amounts', 'wei'])\n",
        "        crypto_value_eth = wei_to_eth(sell_price_wei_str)\n",
        "\n",
        "        # Currency Overrides\n",
        "        if reference_currency == \"WEI\":\n",
        "            fiat_value_gbp = \"-\"\n",
        "        elif reference_currency == \"GBP\":\n",
        "            crypto_value_eth = \"-\"\n",
        "\n",
        "        # Date/Time\n",
        "        full_date_time = tx.get('date', 'N/A')\n",
        "        if 'T' in full_date_time:\n",
        "            transaction_date, transaction_time_z = full_date_time.split('T')\n",
        "            transaction_time = transaction_time_z.rstrip('Z')\n",
        "        else:\n",
        "            transaction_date = full_date_time\n",
        "            transaction_time = 'N/A'\n",
        "\n",
        "        # Map to Dictionary\n",
        "        row_dict = {\n",
        "            \"transaction_type\": tx_type,\n",
        "            \"card_slug\": main_card_slug,\n",
        "            \"card_url\": card_url,\n",
        "            \"sender_slug\": sender,\n",
        "            \"actual_receiver_slug\": receiver,\n",
        "            \"token_op_type\": op_type,\n",
        "            \"reward_id\": reward_id, # <-- ADDED HERE\n",
        "            \"fiat_value_gbp\": fiat_value_gbp,\n",
        "            \"crypto_value_eth\": crypto_value_eth,\n",
        "            \"reference_currency\": reference_currency,\n",
        "            \"cards_exchanged_slugs\": cards_exchanged_str,\n",
        "            \"transaction_date\": transaction_date,\n",
        "            \"transaction_time\": transaction_time,\n",
        "            \"status\": aasm_state,\n",
        "            \"pl_value_gbp\": 0.0,\n",
        "            \"current_time\": current_time,\n",
        "            \"account_entry_id\": account_entry_id,\n",
        "            \"token_operation_id\": token_operation_id # <--- MAPPED HERE\n",
        "        }\n",
        "\n",
        "        row_list = [row_dict.get(h, '') for h in headers]\n",
        "        transformed_data.append(row_list)\n",
        "\n",
        "    return transformed_data\n",
        "\n",
        "\n",
        "# --- UPDATED ANOMALIES WRITER FUNCTION (Cleaned Syntax and returns data) ---\n",
        "def prepare_anomaly_data(anomaly_list, user_slug, headers):\n",
        "    \"\"\"\n",
        "    Transforms anomaly data and prepares it for the batch update.\n",
        "    Returns: Tuple of (headers, data_rows)\n",
        "    \"\"\"\n",
        "    # 1. Use the standard transformation logic\n",
        "    rows_to_write = transform_transaction_data(anomaly_list, user_slug, headers)\n",
        "\n",
        "    # 2. Add an ANOMALY flag to the transaction_type column for clarity\n",
        "    TX_TYPE_INDEX = headers.index('transaction_type')\n",
        "    for row in rows_to_write:\n",
        "        if row and len(row) > TX_TYPE_INDEX:\n",
        "            row[TX_TYPE_INDEX] = \"ANOMALY: \" + str(row[TX_TYPE_INDEX])\n",
        "\n",
        "    if rows_to_write:\n",
        "        print(f\"Prepared {len(rows_to_write)} ANOMALOUS records for batch write.\")\n",
        "    else:\n",
        "        print(\"No anomalous records found to prepare.\")\n",
        "\n",
        "    return (headers, rows_to_write) # Return the data structure needed for batch write\n",
        "\n",
        "\n",
        "# --- UPDATED FUNCTION TO SPLIT DATA BY TRANSACTION TYPE (Added Rewards) ---\n",
        "def split_transactions_by_type(transformed_data):\n",
        "    \"\"\"\n",
        "    Splits the final transformed list into Sales, Buys, and Rewards.\n",
        "    \"\"\"\n",
        "    sales = []\n",
        "    buys = []\n",
        "    rewards = [] # <-- New list for rewards\n",
        "\n",
        "    buy_types = [\n",
        "        \"BUY (Auction)\", \"BUY (Offer)\", \"BUY (Part Exchange)\",\n",
        "        \"BUY (Adjusted)\", \"BUY (Offer Sent)\", \"BUY (Instant)\"\n",
        "    ]\n",
        "    reward_types = [\"REWARD (Monetary)\", \"REWARD (Legacy)\"]\n",
        "\n",
        "    for row in transformed_data:\n",
        "        if not row:\n",
        "            continue\n",
        "\n",
        "        transaction_type = row[0].replace(\"ANOMALY: \", \"\")\n",
        "\n",
        "        if transaction_type == \"SALE\":\n",
        "            sales.append(row)\n",
        "        elif transaction_type in buy_types:\n",
        "            buys.append(row)\n",
        "        elif transaction_type in reward_types: # <-- New reward filter\n",
        "            rewards.append(row)\n",
        "\n",
        "    return sales, buys, rewards # <-- Returns three lists\n",
        "\n",
        "# --- KEEP THESE WRITERS FROM PREVIOUS STEPS (write_raw_data, write_standardized_raw_data, write_to_google_sheet) ---\n",
        "# NOTE: Ensure you copy all three of these functions from your previous working code into this cell as well.\n",
        "# They rely on 'get_nested' and 'gc' (Google Sheets client)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2Oze7IYf4Fh-"
      },
      "outputs": [],
      "source": [
        "# Cell 13: REWARD TRANSFORMATION LOGIC\n",
        "\n",
        "def transform_rewards_card_data(rewarded_rankings, user_slug, headers):\n",
        "    transformed_card_rewards = []\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    for ranking in rewarded_rankings:\n",
        "        # Extract the list of rewards associated with this ranking\n",
        "        so5_rewards_list = ranking.get(\"so5Rewards\", [])\n",
        "\n",
        "        for so5_reward in so5_rewards_list:\n",
        "\n",
        "            reward_cards = so5_reward.get(\"rewardCards\", [])\n",
        "\n",
        "            # --- FILTER: ONLY PROCESS CARD REWARDS ---\n",
        "            if not reward_cards:\n",
        "                continue\n",
        "\n",
        "            # --- Extract Common Ranking/Reward Data ---\n",
        "            reward_slug = so5_reward.get(\"slug\", \"N/A\")\n",
        "            end_date_full = get_nested(so5_reward, [\"so5Leaderboard\", \"endDate\"], \"N/A\")\n",
        "\n",
        "            transaction_date = end_date_full.split('T')[0] if 'T' in end_date_full else end_date_full\n",
        "            transaction_time = end_date_full.split('T')[1].rstrip('Z') if 'T' in end_date_full else 'N/A'\n",
        "\n",
        "            # Get the list of detailed reward types (Monetary, Card, etc.)\n",
        "            detailed_rewards = so5_reward.get(\"rewards\", [])\n",
        "\n",
        "            # --- Process Each Card within the Reward ---\n",
        "            for card_entry in reward_cards:\n",
        "                main_card_slug = get_nested(card_entry, [\"anyCard\", \"slug\"], \"\")\n",
        "                if not main_card_slug:\n",
        "                    continue # Should not happen if filtered correctly\n",
        "\n",
        "                # Find the corresponding card reward ID and status (optional)\n",
        "                reward_id = \"N/A\"\n",
        "                card_status = \"CONFIRMED\"\n",
        "\n",
        "                for reward_detail in detailed_rewards:\n",
        "                    if reward_detail.get(\"__typename\") == \"AnyCardReward\":\n",
        "                        reward_id = reward_detail.get(\"id\", \"N/A\")\n",
        "                        card_status = reward_detail.get(\"aasmState\", \"CONFIRMED\")\n",
        "                        break\n",
        "\n",
        "                # --- Map to AccountTransactions Headers ---\n",
        "                card_url = f\"https://sorare.com/cards/{main_card_slug}\"\n",
        "\n",
        "                row_dict = {\n",
        "                    \"transaction_type\": \"REWARD (Card)\",\n",
        "                    \"card_slug\": main_card_slug,\n",
        "                    \"card_url\": card_url,\n",
        "                    \"sender_slug\": \"Sorare\",\n",
        "                    \"actual_receiver_slug\": user_slug,\n",
        "                    \"token_op_type\": \"TokenReward\",\n",
        "                    \"reward_id\": reward_id,\n",
        "                    \"reward_slug\": reward_slug, # <-- New temporary field to map\n",
        "                    \"fiat_value_gbp\": \"-\",\n",
        "                    \"crypto_value_eth\": \"-\",\n",
        "                    \"reference_currency\": \"-\",\n",
        "                    \"cards_exchanged_slugs\": \"\",\n",
        "                    \"transaction_date\": transaction_date,\n",
        "                    \"transaction_time\": transaction_time,\n",
        "                    \"status\": card_status,\n",
        "                    \"pl_value_gbp\": \"\",\n",
        "                    \"current_time\": current_time,\n",
        "                    \"account_entry_id\": f\"RewardEntry:{reward_id}\",\n",
        "                    \"token_operation_id\": reward_id # <--- USING REWARD ID\n",
        "                }\n",
        "\n",
        "                # Map the dictionary to a list in the correct header order\n",
        "                row_list = [row_dict.get(h, '') for h in headers]\n",
        "                transformed_card_rewards.append(row_list)\n",
        "\n",
        "    return transformed_card_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "XrZW4ObgLS1Q"
      },
      "outputs": [],
      "source": [
        "# Cell 14: CRAFT TRANSFORMATION LOGIC (Card URL Set to Blank)\n",
        "\n",
        "def transform_craft_data(craft_transactions, user_slug, headers):\n",
        "    transformed_craft_rewards = []\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    for craft_tx in craft_transactions:\n",
        "        # 1. FILTER: Only process \"Craft\" transactions\n",
        "        if craft_tx.get(\"description\") != \"Craft\":\n",
        "            continue\n",
        "\n",
        "        # 2. EXTRACT CARD SLUG (Card:GUID) from the ID field\n",
        "        # Format is: \"CardShardsHistoryTransaction:Card:GUID:DATE_TIME:\"\n",
        "        full_id = craft_tx.get(\"id\", \"\")\n",
        "\n",
        "        main_card_slug = \"N/A\"\n",
        "\n",
        "        if full_id:\n",
        "            try:\n",
        "                # Step A: Split by ':'\n",
        "                parts = full_id.split(':')\n",
        "\n",
        "                # Step B: Ensure the necessary parts exist (parts[2] should be the GUID)\n",
        "                if len(parts) >= 3 and parts[2]:\n",
        "                    # Check for \"Card\" prefix and validate GUID length\n",
        "                    if parts[1] == \"Card\" and len(parts[2]) == 36:\n",
        "                        raw_guid = parts[2]\n",
        "                        main_card_slug = \"Card:\" + raw_guid  # Final format: Card:GUID\n",
        "                    else:\n",
        "                        main_card_slug = \"N/A\"\n",
        "\n",
        "                else:\n",
        "                  main_card_slug = \"N/A\"\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting card GUID from {full_id}: {e}\")\n",
        "                main_card_slug = \"N/A\"\n",
        "\n",
        "        # 3. DATE/TIME\n",
        "        full_date_time = craft_tx.get('date', 'N/A')\n",
        "        transaction_date = full_date_time.split('T')[0] if 'T' in full_date_time else full_date_time\n",
        "        transaction_time = full_date_time.split('T')[1].rstrip('Z') if 'T' in full_date_time else 'N/A'\n",
        "\n",
        "\n",
        "        # 4. MAP TO ACCOUNTTRANSACTIONS HEADERS\n",
        "        # FIX: Set card_url to an empty string (\"\") as requested.\n",
        "        card_url = \"\"\n",
        "\n",
        "        row_dict = {\n",
        "            \"transaction_type\": \"CRAFT REWARD (Card)\",\n",
        "            \"card_slug\": main_card_slug,\n",
        "            \"card_url\": card_url,\n",
        "            \"sender_slug\": \"Sorare\",\n",
        "            \"actual_receiver_slug\": user_slug,\n",
        "            \"token_op_type\": \"TokenCraft\",\n",
        "            \"reward_id\": full_id,\n",
        "            \"reward_slug\": craft_tx.get(\"label\", \"\"),\n",
        "            \"fiat_value_gbp\": \"-\",\n",
        "            \"crypto_value_eth\": \"-\",\n",
        "            \"reference_currency\": \"-\",\n",
        "            \"cards_exchanged_slugs\": \"\",\n",
        "            \"transaction_date\": transaction_date,\n",
        "            \"transaction_time\": transaction_time,\n",
        "            \"status\": \"Crafted\",\n",
        "            \"pl_value_gbp\": \"\",\n",
        "            \"current_time\": current_time,\n",
        "            \"account_entry_id\": craft_tx.get(\"id\", \"\"),\n",
        "            \"token_operation_id\": craft_tx.get(\"id\", \"\") # <--- USING CRAFT ID\n",
        "        }\n",
        "\n",
        "        # Map the dictionary to a list in the correct header order\n",
        "        row_list = [row_dict.get(h, '') for h in headers]\n",
        "        transformed_craft_rewards.append(row_list)\n",
        "\n",
        "    return transformed_craft_rewards\n",
        "\n",
        "def write_raw_crafts_dump(craft_transactions, sheet_name, raw_crafts_worksheet_name):\n",
        "    \"\"\"Writes the RAW JSON string of ALL crafts history to a dedicated sheet for auditing.\"\"\"\n",
        "    import json\n",
        "    raw_headers = [\"ID\", \"Date\", \"Description\", \"Raw JSON Data\"]\n",
        "\n",
        "    try:\n",
        "        sh = gc.open(sheet_name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        print(f\"Spreadsheet '{sheet_name}' not found. Skipping raw crafts dump.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        wks_raw = sh.worksheet(raw_crafts_worksheet_name)\n",
        "        wks_raw.clear() # Clear existing content if sheet is found\n",
        "    except gspread.WorksheetNotFound:\n",
        "        wks_raw = sh.add_worksheet(title=raw_crafts_worksheet_name, rows=\"100\", cols=\"4\")\n",
        "\n",
        "    # Prepare data for overwrite\n",
        "    rows_to_write = []\n",
        "    for i, tx in enumerate(craft_transactions):\n",
        "        transaction_id = tx.get('id', f'ROW_{i+1}')\n",
        "        raw_json_string = json.dumps(tx)\n",
        "\n",
        "        # Include key auditing fields for easy sorting in the sheet\n",
        "        rows_to_write.append([\n",
        "            transaction_id,\n",
        "            tx.get('date', 'N/A'),\n",
        "            tx.get('description', 'N/A'),\n",
        "            raw_json_string\n",
        "        ])\n",
        "\n",
        "    data_to_overwrite = [raw_headers] + rows_to_write\n",
        "\n",
        "    if data_to_overwrite:\n",
        "        num_rows = len(data_to_overwrite)\n",
        "        num_cols = len(raw_headers)\n",
        "        range_label = f'A1:{gspread.utils.rowcol_to_a1(num_rows, num_cols)}'\n",
        "\n",
        "        wks_raw.update(range_name=range_label, values=data_to_overwrite, value_input_option='RAW')\n",
        "\n",
        "        print(f\"Successfully OVERWROTE {len(rows_to_write)} rows of RAW CRAFTS HISTORY data to '{raw_crafts_worksheet_name}'.\")\n",
        "    else:\n",
        "        wks_raw.clear()\n",
        "        wks_raw.append_row(raw_headers)\n",
        "        print(\"No raw crafts history data to dump. Headers were refreshed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "voOdfvnzsRjV"
      },
      "outputs": [],
      "source": [
        "# Cell 15: DIRECT OFFER TRANSFORMATION AND SPLITTING LOGIC (UPDATED)\n",
        "\n",
        "# Define new sheet name variables for clarity\n",
        "NOTIONAL_SALES_WORKSHEET_NAME = \"Notional_Sales_Auto\"\n",
        "DIRECT_OFFERS_MANUAL_WORKSHEET_NAME = \"Direct_Offers_Manual_Audit\"\n",
        "\n",
        "def transform_all_direct_offers(raw_offers, user_slug, headers):\n",
        "    \"\"\"\n",
        "    Transforms ALL raw accepted direct offer data into a single audit list.\n",
        "    Does NOT perform the final split or fallout classification.\n",
        "    \"\"\"\n",
        "    transformed_data = []\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    for offer in raw_offers:\n",
        "\n",
        "        # --- Initialize Core Variables to avoid NameError ---\n",
        "        user_sent_cards = []\n",
        "        user_received_cards = []\n",
        "        payment_ref_currency = '-'\n",
        "        final_fiat_value = '-'\n",
        "        final_crypto_value = '-'\n",
        "        main_card_slug = ''\n",
        "\n",
        "        # Get raw side data\n",
        "        sender_cards = [c['slug'] for c in get_nested(offer, ['senderSide', 'anyCards'], [])]\n",
        "        receiver_cards = [c['slug'] for c in get_nested(offer, ['receiverSide', 'anyCards'], [])]\n",
        "\n",
        "\n",
        "        # --- Set Flow based on User's Role ---\n",
        "\n",
        "        if offer.get('direction_role') == 'RECEIVED':\n",
        "            # User is the OFFER RECEIVER (Seller). Other party is the SENDER (Buyer).\n",
        "\n",
        "            # User's flow: Cards sent via receiverSide, Money/Cards received via senderSide.\n",
        "            user_sent_cards = receiver_cards\n",
        "            user_received_cards = sender_cards\n",
        "\n",
        "            # Use the SENDER's amounts as the payment received by the user.\n",
        "            payment_fiat_cents = get_nested(offer, ['senderSide', 'amounts', 'gbpCents'], 0)\n",
        "            payment_wei = get_nested(offer, ['senderSide', 'amounts', 'wei'], '0')\n",
        "            payment_ref_currency = get_nested(offer, ['senderSide', 'amounts', 'referenceCurrency'], '-')\n",
        "\n",
        "            # Final values for output columns (Sale is positive cash flow)\n",
        "            final_fiat_value = payment_fiat_cents / 100.0\n",
        "            final_crypto_value = wei_to_eth(payment_wei)\n",
        "\n",
        "            main_card_slug = \", \".join(user_sent_cards) or \", \".join(user_received_cards)\n",
        "\n",
        "        elif offer.get('direction_role') == 'SENT':\n",
        "            # User is the OFFER SENDER (Buyer or Trader). Other party is the RECEIVER.\n",
        "\n",
        "            # User's flow: Cards sent via senderSide, Money/Cards received via receiverSide.\n",
        "            user_sent_cards = sender_cards\n",
        "            user_received_cards = receiver_cards\n",
        "\n",
        "           # Payment (if any) is complex/negative and is handled by manual audit anyway.\n",
        "           # We rely on the AE cross-check. Defaulting cash values to '-' and currency to '-'.\n",
        "            final_fiat_value = '-'\n",
        "            final_crypto_value = '-'\n",
        "            payment_ref_currency = '-'\n",
        "\n",
        "        else: # Should not happen\n",
        "            continue\n",
        "\n",
        "        # Map the offer into a generic audit row\n",
        "        tx_type = f\"OFFER_TRADE ({offer['direction_role']})\"\n",
        "\n",
        "        full_date_time = offer.get('acceptedAt', offer.get('startDate', 'N/A'))\n",
        "        transaction_date = full_date_time.split('T')[0] if 'T' in full_date_time else full_date_time\n",
        "        transaction_time = full_date_time.split('T')[1].rstrip('Z') if 'T' in full_date_time else 'N/A'\n",
        "\n",
        "        sender_final = get_nested(offer, ['sender', 'slug'], 'N/A')\n",
        "        receiver_final = get_nested(offer, ['receiver', 'slug'], 'N/A')\n",
        "\n",
        "\n",
        "        # Determine Cards Exchanged String based on the user's perspective\n",
        "        cards_exchanged_str = f\"SENT: {', '.join(user_sent_cards)} | RECEIVED: {', '.join(user_received_cards)}\"\n",
        "\n",
        "        row_dict = {\n",
        "            \"transaction_type\": tx_type,\n",
        "            \"card_slug\": main_card_slug,\n",
        "            \"card_url\": \"\", # Keep blank as per previous requirement\n",
        "            \"sender_slug\": sender_final,\n",
        "            \"actual_receiver_slug\": receiver_final,\n",
        "            \"token_op_type\": \"TokenOffer\",\n",
        "            \"reward_id\": offer.get('id', ''),\n",
        "            \"reward_slug\": offer['direction_role'],\n",
        "            # The signs on fiat_output/crypto_output now correctly represent the cash flow (Sale = positive)\n",
        "            # 🔥 CRITICAL: Map payment details directly from senderSide (the buyer) 🔥\n",
        "            \"fiat_value_gbp\": final_fiat_value,\n",
        "            \"crypto_value_eth\": final_crypto_value,\n",
        "            \"reference_currency\": payment_ref_currency, # Capture the ACTUAL CURRENCY (EUR, WEI, GBP, etc.)\n",
        "            \"cards_exchanged_slugs\": cards_exchanged_str, # Use corrected string\n",
        "            \"transaction_date\": transaction_date,\n",
        "            \"transaction_time\": transaction_time,\n",
        "            \"status\": offer['status'].upper(),\n",
        "            \"pl_value_gbp\": 0.0,\n",
        "            \"current_time\": current_time,\n",
        "            # Placeholder ID for initial cross-check\n",
        "            \"account_entry_id\": f\"OfferPreCheck:{offer.get('id', '')}\",\n",
        "            \"token_operation_id\": offer.get('id', '')\n",
        "        }\n",
        "        transformed_data.append([row_dict.get(h, '') for h in headers])\n",
        "\n",
        "    return transformed_data\n",
        "\n",
        "\n",
        "def notional_sale_mapper(offer, user_slug, fiat_value_gbp, crypto_value_eth, reference_currency, card_slugs, headers, current_time):\n",
        "    \"\"\"Maps a Notional Sale to the standard transaction format.\"\"\"\n",
        "\n",
        "    # Currency Overrides (as per your request)\n",
        "    if reference_currency not in [\"GBP\", \"WEI\"]:\n",
        "         reference_currency = \"GBP_FOREIGN_NOTIONAL\" # Use a specific flag\n",
        "\n",
        "    # Format fiat/crypto values for output\n",
        "    fiat_output = round(fiat_value_gbp, 2) if fiat_value_gbp > 0 and reference_currency == \"GBP\" else \"-\"\n",
        "    crypto_output = round(crypto_value_eth, 8) if crypto_value_eth > 0 and reference_currency == \"WEI\" else \"-\"\n",
        "    if reference_currency == \"GBP_FOREIGN_NOTIONAL\":\n",
        "        fiat_output = round(fiat_value_gbp, 2) # Assume fiat value is the GBP value for simplicity\n",
        "\n",
        "\n",
        "    full_date_time = offer.get('acceptedAt', offer.get('startDate', 'N/A'))\n",
        "    transaction_date = full_date_time.split('T')[0] if 'T' in full_date_time else full_date_time\n",
        "    transaction_time = full_date_time.split('T')[1].rstrip('Z') if 'T' in full_date_time else 'N/A'\n",
        "\n",
        "    row_dict = {\n",
        "        \"transaction_type\": \"NOTIONAL_SALE\",\n",
        "        \"card_slug\": card_slugs[0], # Only one card for this type\n",
        "        \"card_url\": f\"https://sorare.com/cards/{card_slugs[0]}\",\n",
        "        \"sender_slug\": user_slug, # User is the seller\n",
        "        \"actual_receiver_slug\": get_nested(offer, ['sender', 'slug'], 'N/A'),\n",
        "        \"token_op_type\": \"TokenOffer\",\n",
        "        \"reward_id\": offer.get('id', ''),\n",
        "        \"reward_slug\": \"SALE_VIA_OFFER\",\n",
        "        \"fiat_value_gbp\": fiat_output,\n",
        "        \"crypto_value_eth\": crypto_output,\n",
        "        \"reference_currency\": reference_currency,\n",
        "        \"cards_exchanged_slugs\": \"\",\n",
        "        \"transaction_date\": transaction_date,\n",
        "        \"transaction_time\": transaction_time,\n",
        "        \"status\": \"PROCESSED_NOTIONAL\",\n",
        "        \"pl_value_gbp\": 0.0,\n",
        "        \"current_time\": current_time,\n",
        "        \"account_entry_id\": \"NOTIONAL_SALE\", # Explicitly mark as NOTIONAL for de-dupe\n",
        "        \"token_operation_id\": offer.get('id', '')\n",
        "    }\n",
        "    return [row_dict.get(h, '') for h in headers]\n",
        "\n",
        "\n",
        "def direct_offer_audit_mapper(offer, user_slug, headers, current_time):\n",
        "    \"\"\"Maps a manual/audit direct offer to the standard transaction format.\"\"\"\n",
        "    # ... (Logic from previous Cell 13.5 remains largely the same for AUDIT rows) ...\n",
        "    # Simplified mapping for audit: just grab the overall offer details\n",
        "    tx_type = f\"OFFER_TRADE ({offer['direction_role']})\"\n",
        "\n",
        "    full_date_time = offer.get('acceptedAt', offer.get('startDate', 'N/A'))\n",
        "    transaction_date = full_date_time.split('T')[0] if 'T' in full_date_time else full_date_time\n",
        "    transaction_time = full_date_time.split('T')[1].rstrip('Z') if 'T' in full_date_time else 'N/A'\n",
        "\n",
        "    sender = get_nested(offer, ['sender', 'slug'], 'N/A')\n",
        "    receiver = get_nested(offer, ['receiver', 'slug'], 'N/A')\n",
        "\n",
        "    sender_cards = [c['slug'] for c in get_nested(offer, ['senderSide', 'anyCards'], [])]\n",
        "    receiver_cards = [c['slug'] for c in get_nested(offer, ['receiverSide', 'anyCards'], [])]\n",
        "\n",
        "    # We will let the cross-check apply the FALLOUT status to the account_entry_id later\n",
        "\n",
        "    row_dict = {\n",
        "        \"transaction_type\": tx_type,\n",
        "        \"card_slug\": \", \".join(receiver_cards) or \", \".join(sender_cards),\n",
        "        \"card_url\": \"\",\n",
        "        \"sender_slug\": sender,\n",
        "        \"actual_receiver_slug\": receiver,\n",
        "        \"token_op_type\": \"TokenOffer\",\n",
        "        \"reward_id\": offer.get('id', ''),\n",
        "        \"reward_slug\": offer['direction_role'],\n",
        "        \"fiat_value_gbp\": \"-\",\n",
        "        \"crypto_value_eth\": \"-\",\n",
        "        \"reference_currency\": \"-\",\n",
        "        \"cards_exchanged_slugs\": f\"SENT: {', '.join(sender_cards)} | RECEIVED: {', '.join(receiver_cards)}\",\n",
        "        \"transaction_date\": transaction_date,\n",
        "        \"transaction_time\": transaction_time,\n",
        "        \"status\": offer['status'].upper(), # Use UPPERCASE for clarity\n",
        "        \"pl_value_gbp\": 0.0,\n",
        "        \"current_time\": current_time,\n",
        "        \"account_entry_id\": f\"DirectOffer:{offer.get('id', '')}\", # Default placeholder ID\n",
        "        \"token_operation_id\": offer.get('id', '')\n",
        "    }\n",
        "    return [row_dict.get(h, '') for h in headers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "mGF490lA1hjS"
      },
      "outputs": [],
      "source": [
        "# Cell 16: DIRECT OFFER CROSS-CHECK LOGIC (REVERTED TO DIRECT TOKEN ID LOOKUP)\n",
        "\n",
        "def cross_check_direct_offers(sales_data, buys_data, monetary_rewards_data, manual_audit_data, headers):\n",
        "    \"\"\"\n",
        "    Checks each Offer against main transaction data using the token_operation_id directly.\n",
        "\n",
        "    If a match is found: sets account_entry_id = matching transaction's account_entry_id.\n",
        "    If no match is found: sets account_entry_id = 'FALLOUT'.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    # 1. Map headers to column indices\n",
        "    header_indices = {header: i for i, header in enumerate(headers)}\n",
        "    AE_ID_COL = header_indices.get(\"account_entry_id\")\n",
        "    TO_ID_COL = header_indices.get(\"token_operation_id\")\n",
        "\n",
        "    if AE_ID_COL is None or TO_ID_COL is None:\n",
        "        print(\"Error: Required headers (account_entry_id, token_operation_id) not found.\")\n",
        "        return manual_audit_data\n",
        "\n",
        "    # 2. Consolidate and prepare Financial Data\n",
        "    all_financial_data_rows = sales_data + buys_data + monetary_rewards_data\n",
        "    financial_df = pd.DataFrame(all_financial_data_rows, columns=headers)\n",
        "\n",
        "    # Create lookup map: {token_operation_id: account_entry_id}\n",
        "    # Use drop_duplicates(keep='last') to handle cases where a single token operation ID\n",
        "    # might map to multiple account entries (common with fees/splits), keeping the most relevant one.\n",
        "    financial_lookup = financial_df.drop_duplicates(\n",
        "        subset=['token_operation_id'], keep='last'\n",
        "    ).set_index('token_operation_id')['account_entry_id'].to_dict()\n",
        "\n",
        "\n",
        "    # 3. Process Direct Offers for Lookup\n",
        "    updated_direct_offers = []\n",
        "\n",
        "    for row in manual_audit_data:\n",
        "\n",
        "        offer_token_op_id = row[TO_ID_COL]\n",
        "\n",
        "        # Perform the DIRECT lookup (no normalization needed)\n",
        "        matched_ae_id = financial_lookup.get(offer_token_op_id)\n",
        "\n",
        "        if matched_ae_id:\n",
        "            # Match found: The offer is covered by a confirmed account entry\n",
        "            row[AE_ID_COL] = matched_ae_id\n",
        "        else:\n",
        "            # No match found: This is a genuine fallout/unrecorded trade\n",
        "            row[AE_ID_COL] = 'FALLOUT'\n",
        "\n",
        "        updated_direct_offers.append(row)\n",
        "\n",
        "    print(f\"Manual Audit Offer Cross-Check complete. Updated {len(updated_direct_offers)} records.\")\n",
        "\n",
        "    return updated_direct_offers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "1b3oeU2tiGVj"
      },
      "outputs": [],
      "source": [
        "# Cell 17: SPLIT FALLOUT DIRECT OFFERS (Updated with URL Generation)\n",
        "\n",
        "def split_fallout_direct_offers(all_offers_list, user_slug, headers):\n",
        "    \"\"\"\n",
        "    Splits the full list of cross-checked offers (where fallout has been applied)\n",
        "    into Notional Sales (FALLOUT + Sale Pattern) and Manual Audit (FALLOUT + Others / PROCESSED).\n",
        "    \"\"\"\n",
        "    notional_sales_data = []\n",
        "    final_manual_audit_data = [] # Retains processed, fallout, and general audit entries\n",
        "\n",
        "    # Map headers to column indices\n",
        "    header_indices = {header: i for i, header in enumerate(headers)}\n",
        "    AE_ID_COL = header_indices.get(\"account_entry_id\")\n",
        "    TO_ID_COL = header_indices.get(\"token_operation_id\")\n",
        "    TX_TYPE_COL = header_indices.get(\"transaction_type\")\n",
        "    CARDS_EXCHANGED_COL = header_indices.get(\"cards_exchanged_slugs\")\n",
        "    CARD_SLUG_COL = header_indices.get(\"card_slug\") # NEW INDEX\n",
        "    CARD_URL_COL = header_indices.get(\"card_url\") # NEW INDEX\n",
        "\n",
        "    # Check for required headers\n",
        "    if AE_ID_COL is None or TO_ID_COL is None or TX_TYPE_COL is None or CARD_SLUG_COL is None or CARD_URL_COL is None:\n",
        "        print(\"Error: Required headers not found for splitting. Returning all to Manual Audit.\")\n",
        "        return [], all_offers_list\n",
        "\n",
        "    # Get indexes for currency/status updates\n",
        "    fiat_col = header_indices.get(\"fiat_value_gbp\")\n",
        "    crypto_col = header_indices.get(\"crypto_value_eth\")\n",
        "    ref_col = header_indices.get(\"reference_currency\")\n",
        "\n",
        "    for row in all_offers_list:\n",
        "\n",
        "        # 1. Check if it's a FALLOUT\n",
        "        is_fallout = (row[AE_ID_COL] == 'FALLOUT')\n",
        "\n",
        "        # 2. Check for Notional Sale Pattern (Only execute if it's a FALLOUT)\n",
        "\n",
        "        cards_exchanged_string = row[CARDS_EXCHANGED_COL]\n",
        "        is_notional_sale_pattern = False # Initialize to False\n",
        "\n",
        "        if is_fallout:\n",
        "\n",
        "            try:\n",
        "                sent_part, received_part = cards_exchanged_string.split(' | RECEIVED: ')\n",
        "\n",
        "                cards_sent_slugs_str = sent_part.replace('SENT: ', '').strip()\n",
        "                cards_received_slugs_str = received_part.strip()\n",
        "\n",
        "                # A. Check for exactly ONE card SENT by the user (sold card)\n",
        "                user_sent_one_card = (cards_sent_slugs_str != '' and ',' not in cards_sent_slugs_str)\n",
        "\n",
        "                # B. Check for ZERO cards RECEIVED by the user\n",
        "                user_received_zero_cards = (cards_received_slugs_str == '')\n",
        "\n",
        "                is_notional_sale_pattern = user_sent_one_card and user_received_zero_cards\n",
        "\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "\n",
        "        if is_notional_sale_pattern:\n",
        "            # --- Map to Notional Sale ---\n",
        "\n",
        "            # 1. Generate Card URL (Remains the same)\n",
        "            card_slug = row[CARD_SLUG_COL]\n",
        "            if card_slug and ' ' not in card_slug:\n",
        "                 row[CARD_URL_COL] = f\"https://sorare.com/cards/{card_slug}\"\n",
        "\n",
        "            # Get the current values and reference currency from the row\n",
        "            original_ref_currency = row[ref_col]\n",
        "            current_fiat_value = row[fiat_col]\n",
        "            # current_crypto_value is already in ETH from Cell 15\n",
        "            current_crypto_value = row[crypto_col]\n",
        "\n",
        "            # --- 🔥 2. Apply Exclusive Currency Rules (The Final Fix) 🔥\n",
        "\n",
        "            if original_ref_currency == \"WEI\":\n",
        "                # Rule (1): WEI\n",
        "                row[ref_col] = \"WEI\"\n",
        "                row[fiat_col] = \"-\"                   # Dash out Fiat\n",
        "                # row[crypto_col] remains as is (populated with ETH)\n",
        "\n",
        "            elif original_ref_currency == \"GBP\":\n",
        "                # Rule (2): GBP\n",
        "                row[ref_col] = \"GBP\"\n",
        "                row[crypto_col] = \"-\"                 # Dash out Crypto\n",
        "                # row[fiat_col] remains as is (populated with GBP amount)\n",
        "\n",
        "            else:\n",
        "                # Rule (3): GBP_FOREIGN (EUR, USD, etc.)\n",
        "                row[ref_col] = \"GBP_FOREIGN\"\n",
        "                row[crypto_col] = \"-\"                 # Dash out Crypto\n",
        "                # row[fiat_col] remains as is (populated with the GBP Cents equivalent)\n",
        "\n",
        "\n",
        "            # 3. Finalize Status and IDs\n",
        "            row[TX_TYPE_COL] = \"NOTIONAL_SALE\"\n",
        "            row[header_indices.get(\"status\")] = \"PROCESSED_NOTIONAL\"\n",
        "            row[AE_ID_COL] = \"NOTIONAL_SALE_MAPPED\"\n",
        "\n",
        "            # Append to sales\n",
        "            notional_sales_data.append(row)\n",
        "        else:\n",
        "            # --- Map to Manual Audit ---\n",
        "            final_manual_audit_data.append(row)\n",
        "\n",
        "    print(f\"Split complete: {len(notional_sales_data)} Notional Sales isolated. {len(final_manual_audit_data)} remaining for Manual Audit.\")\n",
        "\n",
        "    return notional_sales_data, final_manual_audit_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "MZuYwnpxEkQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff0b4dd-35c2-45d3-8e84-95c3d783bf0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting API Fetches and Initial Processing ---\n",
            "Fetching rewards data...\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Status Code: 200\n",
            "Total rewards entries fetched: 4003\n",
            "\n",
            "Fetching crafted card data for all rarities...\n",
            "  -> Fetching unique crafts...\n",
            "  -> Fetching super_rare crafts...\n",
            "  -> Fetching rare crafts...\n",
            "  -> Fetching limited crafts...\n",
            "Total crafted transactions fetched: 1047\n",
            "\n",
            "Fetching ACCEPTED direct offer data for SENT and RECEIVED directions...\n",
            "  -> Fetching SENT offers...\n",
            "  -> Found 181 accepted SENT offers.\n",
            "  -> Fetching RECEIVED offers...\n",
            "  -> Found 788 accepted RECEIVED offers.\n",
            "Total accepted direct offers fetched: 969\n",
            "Manual Audit Offer Cross-Check complete. Updated 969 records.\n",
            "Split complete: 697 Notional Sales isolated. 272 remaining for Manual Audit.\n",
            "--- Initial Data Processing Complete ---\n"
          ]
        }
      ],
      "source": [
        "# SCRIPT 1: Cell 17.1 (Fetch and Initial Processing)\n",
        "\n",
        "if 'transactions' in locals() and transactions:\n",
        "    print(\"--- Starting API Fetches and Initial Processing ---\")\n",
        "\n",
        "    # 1. Fetch auxiliary data\n",
        "    rewards_rankings = fetch_all_rewards(rewards_query, user, api_key, jwt)\n",
        "    craft_transactions = fetch_all_crafts(crafts_query, api_key, jwt)\n",
        "    direct_offers = fetch_all_direct_offers(direct_offers_query, user, api_key, jwt)\n",
        "\n",
        "    # 2. Filter and Process Main Data\n",
        "    main_transactions, anomaly_transactions = filter_transactions(transactions)\n",
        "    data_to_write = transform_transaction_data(main_transactions, user, headers)\n",
        "    sales_data, buys_data, monetary_rewards_data = split_transactions_by_type(data_to_write)\n",
        "\n",
        "    # 3. Process Auxiliary Data\n",
        "    card_rewards_data = transform_rewards_card_data(rewards_rankings, user, headers)\n",
        "    crafted_data = transform_craft_data(craft_transactions, user, headers)\n",
        "\n",
        "    # 4. Process Direct Offers\n",
        "    all_direct_offers_audit_data = transform_all_direct_offers(direct_offers, user, headers)\n",
        "    all_direct_offers_audit_data = cross_check_direct_offers(\n",
        "        sales_data, buys_data, monetary_rewards_data, all_direct_offers_audit_data, headers\n",
        "    )\n",
        "    notional_sales_data, direct_offers_manual_audit_data = split_fallout_direct_offers(\n",
        "        all_direct_offers_audit_data, user, headers\n",
        "    )\n",
        "    print(\"--- Initial Data Processing Complete ---\")\n",
        "else:\n",
        "    print(\"Error: 'transactions' list is empty or not defined. Rerun API fetch cells.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "bDo3ChOPExmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e227bd82-3133-4d18-fe46-80c5c46426a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Audit and Sorting ---\n",
            "Loaded 12 manually resolved trades from 'Notional_Sales_Manual'.\n",
            "Flagged 12 records in 'Direct_Offers_Manual_Audit' as resolved.\n",
            "Sorted Notional Sales and Manual Audit by date (newest first).\n"
          ]
        }
      ],
      "source": [
        "# SCRIPT 1: Cell 17.2 (Processing and Audit Trail)\n",
        "\n",
        "if 'sales_data' in locals():\n",
        "    print(\"--- Starting Audit and Sorting ---\")\n",
        "\n",
        "    # Get the index of the 'token_operation_id', 'status', and 'account_entry_id' columns\n",
        "    TO_ID_INDEX = headers.index('token_operation_id')\n",
        "    STATUS_INDEX = headers.index('status')\n",
        "    DATE_INDEX = headers.index('transaction_date')\n",
        "    AE_ID_INDEX = headers.index('account_entry_id')\n",
        "\n",
        "    # 5. Flag resolved trades in Direct_Offers_Manual_Audit\n",
        "\n",
        "    # A. Load the list of manually resolved Token Operation IDs\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        wks_manual = gc.open(GOOGLE_SHEET_NAME).worksheet(MANUAL_SALES_WORKSHEET_NAME)\n",
        "        manual_sales_df = pd.DataFrame(wks_manual.get_all_records())\n",
        "        resolved_token_ids = set(manual_sales_df['token_operation_id'].astype(str).tolist())\n",
        "        print(f\"Loaded {len(resolved_token_ids)} manually resolved trades from '{MANUAL_SALES_WORKSHEET_NAME}'.\")\n",
        "    except Exception as e:\n",
        "        resolved_token_ids = set()\n",
        "        print(f\"Error loading manual sales data for flagging: {e}. Skipping flag.\")\n",
        "\n",
        "\n",
        "    # B. Update the status in the manual audit list\n",
        "    resolved_count = 0\n",
        "    for row in direct_offers_manual_audit_data:\n",
        "        token_op_id = str(row[TO_ID_INDEX])\n",
        "\n",
        "        is_fallout = row[AE_ID_INDEX] == 'FALLOUT'\n",
        "\n",
        "        if is_fallout and token_op_id in resolved_token_ids:\n",
        "             row[STATUS_INDEX] = \"RESOLVED_TO_MANUAL_SALE\"\n",
        "             row[AE_ID_INDEX] = \"MANUAL_SALE_RESOLVED\"\n",
        "             resolved_count += 1\n",
        "\n",
        "    print(f\"Flagged {resolved_count} records in '{DIRECT_OFFERS_MANUAL_WORKSHEET_NAME}' as resolved.\")\n",
        "\n",
        "\n",
        "    # 6. SORTING LOGIC\n",
        "    notional_sales_data.sort(key=lambda x: x[DATE_INDEX], reverse=True)\n",
        "    direct_offers_manual_audit_data.sort(key=lambda x: x[DATE_INDEX], reverse=True)\n",
        "    print(\"Sorted Notional Sales and Manual Audit by date (newest first).\")\n",
        "\n",
        "else:\n",
        "    print(\"Run Cell 17.1 first to generate data structures.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "fChN25TuE1Mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf203dfc-0b2e-42cb-dab2-8312f25ab324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Batch Write and Snapshot ---\n",
            "-> Prepared 2878 rows for 'AccountTransactions_Sales'.\n",
            "-> Prepared 2586 rows for 'AccountTransactions_Buys'.\n",
            "-> Prepared 1635 rows for 'AccountTransactions_MonetaryRewards'.\n",
            "-> Prepared 3202 rows for 'MyRewards_Cards'.\n",
            "-> Prepared 90 rows for 'MyCrafts_Cards'.\n",
            "-> Prepared 697 rows for 'Notional_Sales_Auto'.\n",
            "-> Prepared 272 rows for 'Direct_Offers_Manual_Audit'.\n",
            "\n",
            "✅ Successfully executed BATCH UPDATE for all sheets.\n",
            "Successfully OVERWROTE 65 ANOMALOUS records to 'Anomalies_Excluded_Data'.\n",
            "Drive API service initialized.\n",
            "✅ Snapshot 'Sorare Transactions Dump_Snapshot_20251027_093624' successfully created and stored in 'SorareReportsSnapshots'.\n"
          ]
        }
      ],
      "source": [
        "# SCRIPT 1: Cell 18 (Batch Write and Snapshot Archival)\n",
        "\n",
        "if 'sales_data' in locals():\n",
        "    print(\"--- Starting Batch Write and Snapshot ---\")\n",
        "\n",
        "    # 7. FULL BATCH WRITE SETUP\n",
        "    all_sheets_data = {}\n",
        "\n",
        "    # 8. CONDITIONAL RAW DUMPS (Logic copied from previous step)\n",
        "    import json\n",
        "    # Ensure RUN_RAW_DUMPS is available (defined in a previous cell)\n",
        "    if 'RUN_RAW_DUMPS' not in locals(): RUN_RAW_DUMPS = False\n",
        "\n",
        "    if RUN_RAW_DUMPS:\n",
        "        print(\"Preparing RAW Dumps...\")\n",
        "        # --- Raw JSON Dump ---\n",
        "        raw_json_headers = [\"ID\", \"Raw JSON Data\"]\n",
        "        raw_json_data = [[tx.get('id', f'ROW_{i+1}'), json.dumps(tx)] for i, tx in enumerate(transactions)]\n",
        "        all_sheets_data[RAW_WORKSHEET_NAME] = (raw_json_headers, raw_json_data)\n",
        "\n",
        "        # --- Standardized Raw Dump (Headers and data must be derived as before) ---\n",
        "        # ... (Insert full logic for Standardized Raw Dump preparation here) ...\n",
        "\n",
        "        # --- Raw Crafts Audit Dump (Headers and data must be derived as before) ---\n",
        "        # ... (Insert full logic for Raw Crafts Audit Dump preparation here) ...\n",
        "\n",
        "    # 9. Add the main, standard sheets\n",
        "    all_sheets_data[SALES_WORKSHEET_NAME] = (headers, sales_data)\n",
        "    all_sheets_data[BUYS_WORKSHEET_NAME] = (headers, buys_data)\n",
        "    all_sheets_data[REWARDS_WORKSHEET_NAME] = (headers, monetary_rewards_data)\n",
        "    all_sheets_data[CARD_REWARDS_WORKSHEET_NAME] = (headers, card_rewards_data)\n",
        "    all_sheets_data[CRAFTED_CARDS_WORKSHEET_NAME] = (headers, crafted_data)\n",
        "    all_sheets_data[NOTIONAL_SALES_WORKSHEET_NAME] = (headers, notional_sales_data)\n",
        "    all_sheets_data[DIRECT_OFFERS_MANUAL_WORKSHEET_NAME] = (headers, direct_offers_manual_audit_data)\n",
        "\n",
        "    # 10. Execute Batch Write\n",
        "    write_in_batch(GOOGLE_SHEET_NAME, all_sheets_data)\n",
        "\n",
        "    # 11. Write Anomalies\n",
        "    write_anomalies(anomaly_transactions, GOOGLE_SHEET_NAME, ANOMALIES_WORKSHEET_NAME, user, headers)\n",
        "\n",
        "    # 12. ARCHIVAL SNAPSHOT\n",
        "    try:\n",
        "        main_sh = gc.open(GOOGLE_SHEET_NAME)\n",
        "        snapshot_folder_id = get_snapshot_folder_id(SNAPSHOT_FOLDER_NAME)\n",
        "\n",
        "        if snapshot_folder_id:\n",
        "            create_and_store_snapshot(\n",
        "                source_file_id=main_sh.id,\n",
        "                source_file_name=GOOGLE_SHEET_NAME,\n",
        "                target_folder_id=snapshot_folder_id\n",
        "            )\n",
        "        else:\n",
        "            print(\"Snapshot skipped due to folder access error.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to locate or snapshot the main workbook: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Run Cell 17.1 first to generate data structures.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwJHEDRsMXGnTcvVb3dh0I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}